{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gotero/DeepLearningLifeSciences/blob/master/primed_llm___running.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Uninstall the conflicting Google/TensorFlow monitoring tools\n",
        "# We do not need these for PyTorch LLM training\n",
        "!pip uninstall -y opentelemetry-api opentelemetry-sdk opentelemetry-proto google-generativeai tensorflow-metadata grpcio-status\n",
        "\n",
        "# 2. Uninstall our core libraries to start fresh\n",
        "!pip uninstall -y transformers peft trl accelerate bitsandbytes protobuf sentence-transformers\n",
        "\n",
        "# 3. Install the \"Golden Compatibility\" Stack\n",
        "# We install these together so pip can try to resolve them as a group\n",
        "!pip install -q \\\n",
        "    \"protobuf==3.20.3\" \\\n",
        "    \"torch\" \\\n",
        "    \"transformers==4.40.0\" \\\n",
        "    \"peft==0.10.0\" \\\n",
        "    \"bitsandbytes==0.43.1\" \\\n",
        "    \"accelerate==0.29.3\" \\\n",
        "    \"trl==0.8.6\" \\\n",
        "    \"sentence-transformers==2.7.0\" \\\n",
        "    \"pandas\" \\\n",
        "    \"numpy\" \\\n",
        "    \"pandera\" \\\n",
        "    \"ydata-profiling\" \\\n",
        "    \"scikit-learn\" \\\n",
        "    \"skrub\" \\\n",
        "    \"openpyxl\" \\\n",
        "    \"scipy\"\n",
        "\n",
        "# 4. Force install Triton (Critical for GPU)\n",
        "!pip install -q -U triton"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wJZu0qb8UDb9",
        "outputId": "958db8f9-6b8e-4a92-b1fd-d792cd5c76c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: opentelemetry-api 1.37.0\n",
            "Uninstalling opentelemetry-api-1.37.0:\n",
            "  Successfully uninstalled opentelemetry-api-1.37.0\n",
            "Found existing installation: opentelemetry-sdk 1.37.0\n",
            "Uninstalling opentelemetry-sdk-1.37.0:\n",
            "  Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "Found existing installation: opentelemetry-proto 1.37.0\n",
            "Uninstalling opentelemetry-proto-1.37.0:\n",
            "  Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "Found existing installation: google-generativeai 0.8.5\n",
            "Uninstalling google-generativeai-0.8.5:\n",
            "  Successfully uninstalled google-generativeai-0.8.5\n",
            "Found existing installation: tensorflow-metadata 1.17.2\n",
            "Uninstalling tensorflow-metadata-1.17.2:\n",
            "  Successfully uninstalled tensorflow-metadata-1.17.2\n",
            "Found existing installation: grpcio-status 1.71.2\n",
            "Uninstalling grpcio-status-1.71.2:\n",
            "  Successfully uninstalled grpcio-status-1.71.2\n",
            "Found existing installation: transformers 4.57.3\n",
            "Uninstalling transformers-4.57.3:\n",
            "  Successfully uninstalled transformers-4.57.3\n",
            "Found existing installation: peft 0.18.0\n",
            "Uninstalling peft-0.18.0:\n",
            "  Successfully uninstalled peft-0.18.0\n",
            "\u001b[33mWARNING: Skipping trl as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: accelerate 1.12.0\n",
            "Uninstalling accelerate-1.12.0:\n",
            "  Successfully uninstalled accelerate-1.12.0\n",
            "\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: protobuf 5.29.5\n",
            "Uninstalling protobuf-5.29.5:\n",
            "  Successfully uninstalled protobuf-5.29.5\n",
            "Found existing installation: sentence-transformers 5.1.2\n",
            "Uninstalling sentence-transformers-5.1.2:\n",
            "  Successfully uninstalled sentence-transformers-5.1.2\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m295.9/295.9 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m398.7/398.7 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m478.8/478.8 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m679.7/679.7 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-logging 3.12.1 requires opentelemetry-api>=1.9.0, which is not installed.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-api>=1.35.0, which is not installed.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, which is not installed.\n",
            "tensorflow-datasets 4.9.9 requires tensorflow-metadata, which is not installed.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-api~=1.15, which is not installed.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, which is not installed.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, which is not installed.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.9.0+cu126 requires triton==3.5.0; platform_system == \"Linux\", but you have triton 3.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Uninstall the broken, old version\n",
        "!pip uninstall -y bitsandbytes\n",
        "\n",
        "# 2. Install the latest version which supports Python 3.12 and Triton 3.x\n",
        "!pip install -q -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv8sGVjVajjd",
        "outputId": "01baaacd-62c2-4a3b-8ca7-6b3417845952"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: bitsandbytes 0.43.1\n",
            "Uninstalling bitsandbytes-0.43.1:\n",
            "  Successfully uninstalled bitsandbytes-0.43.1\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "try:\n",
        "    print(f\"BnB Version: {bnb.__version__}\")\n",
        "    # The new version might rely on dynamic loading, so we just check import\n",
        "    print(\"‚úÖ Success: Bitsandbytes loaded!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HcYit1-UOMT",
        "outputId": "c24fdb21-8d2f-48b8-c759-f5f5f604b760"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA: True\n",
            "BnB Version: 0.48.2\n",
            "‚úÖ Success: Bitsandbytes loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYPT-OXs3851",
        "outputId": "c2c8896e-6556-485f-e159-a3819e6a2990"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "cx-3LtK22H8u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "1f377d89913a44aba7702dfff231a93c",
            "9cb99347968f4166ad1903f427fa86cf",
            "beafaa40f4bc4804b34fdd04efb3b989",
            "b08ed52ca9414158abec682dc8e42ac7",
            "f02d187405e4421eaa88f2994743ea7e",
            "ae6d6cb88a6549b088b7c06cded73ae1",
            "946299f94a56493c8caa84f68f983e66",
            "17b217da67784949b5078635b183be32",
            "639c93ebd2214882be8121e5669accd2",
            "8d9f633bd46a42a89f0a2bf722e637bb",
            "e2cca4dd92f041faa8f4d50cfadb82cb",
            "c27673b3ffae47a3966c3029c9dce8c6",
            "8b9c25c9c7d648de8b946a5e652c1b3a",
            "382998b08f85435492856ffbf3c3beec",
            "ced9fbb78f98476490b92fbc32e495cf",
            "c86bc5b814684977b42964836656426a",
            "7a195e330a74422c9beea002b4b07454",
            "394239a6546b48ebb048def1632e9af1",
            "649bdd0cd3a6421eaab3ef2542aa9289",
            "f52316e660eb44e18b7ef8fac139bac3"
          ]
        },
        "outputId": "7a89b87c-5831-41d5-f70b-3edbb2e0b96d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f377d89913a44aba7702dfff231a93c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile clean_data.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import argparse\n",
        "import sys\n",
        "import os\n",
        "import logging\n",
        "from sklearn.impute import KNNImputer\n",
        "from skrub import deduplicate\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def standardize_nulls_and_drop_empty(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    logger.info(\"--- Step 1: Standardizing Nulls ---\")\n",
        "    null_placeholders = [\"\", \" \", \"NA\", \"N/A\", \"null\", \"NULL\", \"-\", \"nan\", \"NaN\"]\n",
        "    df.replace(null_placeholders, np.nan, inplace=True)\n",
        "\n",
        "    empty_cols = df.columns[df.isnull().all()].tolist()\n",
        "    if empty_cols:\n",
        "        logger.info(f\"Dropping empty columns: {empty_cols}\")\n",
        "        df.drop(columns=empty_cols, inplace=True)\n",
        "    return df\n",
        "\n",
        "def fix_gene_excel_errors(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    logger.info(\"--- Step 2: Fixing Gene Name Errors ---\")\n",
        "    excel_gene_map = {\n",
        "        \"1-Mar\": \"MARCH1\", \"1-Sep\": \"SEPT1\", \"2-Mar\": \"MARCH2\", \"2-Sep\": \"SEPT2\",\n",
        "        \"3-Mar\": \"MARCH3\", \"3-Sep\": \"SEPT3\", \"4-Mar\": \"MARCH4\", \"4-Sep\": \"SEPT4\",\n",
        "        \"5-Mar\": \"MARCH5\", \"5-Sep\": \"SEPT5\", \"6-Mar\": \"MARCH6\", \"6-Sep\": \"SEPT6\",\n",
        "        \"7-Mar\": \"MARCH7\", \"7-Sep\": \"SEPT7\", \"8-Mar\": \"MARCH8\", \"8-Sep\": \"SEPT8\",\n",
        "        \"9-Mar\": \"MARCH9\", \"9-Sep\": \"SEPT9\", \"10-Mar\": \"MARCH10\", \"10-Sep\": \"SEPT10\",\n",
        "    }\n",
        "    for col in df.select_dtypes(include='object').columns:\n",
        "        if df[col].astype(str).isin(excel_gene_map.keys()).any():\n",
        "            df[col] = df[col].replace(excel_gene_map)\n",
        "    return df\n",
        "\n",
        "def impute_missing_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    logger.info(\"--- Step 3: Imputing Missing Data ---\")\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if not numeric_cols: return df\n",
        "\n",
        "    # Simple check if imputation is needed\n",
        "    if df[numeric_cols].isnull().sum().sum() > 0:\n",
        "        try:\n",
        "            imputer = KNNImputer(n_neighbors=5)\n",
        "            df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Imputation skipped due to error: {e}\")\n",
        "    return df\n",
        "\n",
        "def save_cleaned_data(df: pd.DataFrame, output_path: str):\n",
        "    df.to_csv(output_path, index=False)\n",
        "    logger.info(f\"‚úÖ Cleaned data saved to: {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"input_file\", help=\"Path to input .csv or .xlsx file\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    input_path = args.input_file\n",
        "    file_dir = os.path.dirname(input_path) or \".\"\n",
        "    file_name = os.path.basename(input_path)\n",
        "    file_root, file_ext = os.path.splitext(file_name)\n",
        "    output_path = os.path.join(file_dir, f\"cleaned_{file_root}.csv\")\n",
        "\n",
        "    logger.info(f\"Processing {input_path}...\")\n",
        "    try:\n",
        "        if file_ext.lower() in ['.xlsx', '.xls']:\n",
        "            df = pd.read_excel(input_path)\n",
        "        else:\n",
        "            df = pd.read_csv(input_path)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error loading file: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    df = standardize_nulls_and_drop_empty(df)\n",
        "    df = fix_gene_excel_errors(df)\n",
        "    df = impute_missing_data(df)\n",
        "    save_cleaned_data(df, output_path)"
      ],
      "metadata": {
        "id": "4-xjASa52NBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "079a7150-126d-4415-9a27-257487808bc8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing clean_data.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile calculate_scores.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def calculate_and_export_scores(input_file):\n",
        "    if not os.path.exists(input_file):\n",
        "        sys.exit(f\"Error: File {input_file} not found.\")\n",
        "\n",
        "    df = pd.read_csv(input_file)\n",
        "    calc_df = df.copy()\n",
        "\n",
        "    # Map Y/N to 1/0 for calculation\n",
        "    def map_binary(val):\n",
        "        if isinstance(val, str):\n",
        "            v = val.lower().strip()\n",
        "            if v in ['y', 'yes']: return 1\n",
        "            if v in ['n', 'no']: return 0\n",
        "        return 0\n",
        "\n",
        "    binary_cols = ['Prior VTE', 'Active Cancer', 'Recent Surgery/Immobilization',\n",
        "                   'Estrogen Use', 'Hemoptysis', 'Leg Swelling/Tenderness', 'Alternative Dx Less Likely']\n",
        "\n",
        "    for col in binary_cols:\n",
        "        if col in calc_df.columns:\n",
        "            calc_df[col] = calc_df[col].apply(map_binary)\n",
        "        else:\n",
        "            calc_df[col] = 0\n",
        "\n",
        "    # Ensure numeric columns\n",
        "    for col in ['Age', 'HR', 'SpO‚ÇÇ (RA)']:\n",
        "        calc_df[col] = pd.to_numeric(calc_df[col], errors='coerce').fillna(0)\n",
        "\n",
        "    # 1. Wells Score\n",
        "    wells = (\n",
        "        (calc_df['Leg Swelling/Tenderness'] * 3.0) + (calc_df['Alternative Dx Less Likely'] * 3.0) +\n",
        "        ((calc_df['HR'] > 100).astype(int) * 1.5) + (calc_df['Recent Surgery/Immobilization'] * 1.5) +\n",
        "        (calc_df['Prior VTE'] * 1.5) + (calc_df['Hemoptysis'] * 1.0) + (calc_df['Active Cancer'] * 1.0)\n",
        "    )\n",
        "\n",
        "    # 2. Geneva Score\n",
        "    geneva_hr = calc_df['HR'].apply(lambda x: 3 if 75 <= x <= 94 else (5 if x > 94 else 0))\n",
        "    geneva = (\n",
        "        ((calc_df['Age'] > 65).astype(int) * 1) + (calc_df['Prior VTE'] * 3) +\n",
        "        (calc_df['Recent Surgery/Immobilization'] * 2) + (calc_df['Active Cancer'] * 2) +\n",
        "        (calc_df['Leg Swelling/Tenderness'] * 3) + (calc_df['Hemoptysis'] * 2) + geneva_hr\n",
        "    )\n",
        "\n",
        "    # 3. PERC Rule\n",
        "    perc = (\n",
        "        ((calc_df['Age'] >= 50).astype(int)) + ((calc_df['HR'] >= 100).astype(int)) +\n",
        "        ((calc_df['SpO‚ÇÇ (RA)'] < 95).astype(int)) + calc_df['Leg Swelling/Tenderness'] +\n",
        "        calc_df['Hemoptysis'] + calc_df['Recent Surgery/Immobilization'] +\n",
        "        calc_df['Prior VTE'] + calc_df['Estrogen Use']\n",
        "    )\n",
        "\n",
        "    # Add scores to original dataframe\n",
        "    df['Wells Score'] = wells\n",
        "    df['Geneva Score'] = geneva\n",
        "    df['PERC Rule'] = perc\n",
        "\n",
        "    # Generate dynamic output filename: cleaned_filename_with_scores.csv\n",
        "    base, ext = os.path.splitext(input_file)\n",
        "    output_file = f\"{base}_with_scores{ext}\"\n",
        "\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Scored data saved to: {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) < 2:\n",
        "        sys.exit(\"Usage: python calculate_scores.py <input_csv>\")\n",
        "    calculate_and_export_scores(sys.argv[1])"
      ],
      "metadata": {
        "id": "j1dIsccX2r4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e60301a-dc5c-4c38-958d-a14a31924099"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing calculate_scores.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile preprocess_data.py\n",
        "import pandas as pd\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def create_training_data(input_file):\n",
        "    if not os.path.exists(input_file):\n",
        "        sys.exit(f\"Error: File {input_file} not found.\")\n",
        "\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    # Dynamic Feature Extraction\n",
        "    target_column = 'CTPA Result'\n",
        "    ignore_cols = ['Subject', 'Study ID', 'Encounter Date', target_column]\n",
        "\n",
        "    feature_columns = [c for c in df.columns if c not in ignore_cols]\n",
        "\n",
        "    def format_instruction(row):\n",
        "        # 1. Build the Input String\n",
        "        features = []\n",
        "        for col in feature_columns:\n",
        "            val = row[col]\n",
        "            if pd.notna(val) and str(val).strip() != \"\":\n",
        "                features.append(f\"{col}: {val}\")\n",
        "\n",
        "        input_text = \"\\n\".join(features)\n",
        "        output_text = str(row[target_column]) if pd.notna(row[target_column]) else \"Unknown\"\n",
        "\n",
        "        # 2. Create the FULL PROMPT immediately\n",
        "        # This matches the BioMistral/Alpaca format\n",
        "        full_text = (\n",
        "            f\"### Instruction: Analyze the patient clinical data and calculate risk scores to predict the CTPA result.\\n\"\n",
        "            f\"### Input: {input_text}\\n\"\n",
        "            f\"### Output: {output_text}\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"text\": full_text,  # <--- The Trainer looks for this specific key\n",
        "            \"instruction\": \"Analyze the patient clinical data...\",\n",
        "            \"input\": input_text,\n",
        "            \"output\": output_text\n",
        "        }\n",
        "\n",
        "    dataset = df.apply(format_instruction, axis=1).tolist()\n",
        "    train_data, val_data = train_test_split(dataset, test_size=0.1, random_state=42)\n",
        "\n",
        "    with open('train_data.jsonl', 'w') as f:\n",
        "        for entry in train_data:\n",
        "            json.dump(entry, f); f.write('\\n')\n",
        "\n",
        "    with open('val_data.jsonl', 'w') as f:\n",
        "        for entry in val_data:\n",
        "            json.dump(entry, f); f.write('\\n')\n",
        "\n",
        "    print(f\"‚úÖ Preprocessing complete. Training samples: {len(train_data)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) < 2:\n",
        "        sys.exit(\"Usage: python preprocess_data.py <input_csv>\")\n",
        "    create_training_data(sys.argv[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVSPQ17LedWZ",
        "outputId": "b79de32f-bfa2-4062-c6b7-41bdf20c7f5e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing preprocess_data.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "def run_auto_pipeline():\n",
        "    print(\"üöÄ Starting Automated Clinical Pipeline...\")\n",
        "\n",
        "    # 1. Auto-detect Input File\n",
        "    # Finds any CSV or XLSX that isn't a result file from a previous run\n",
        "    extensions = [\"*.csv\", \"*.xlsx\"]\n",
        "    potential_files = []\n",
        "    for ext in extensions:\n",
        "        potential_files.extend(glob.glob(ext))\n",
        "\n",
        "    # Filter out files generated by the pipeline itself to avoid loops\n",
        "    input_files = [\n",
        "        f for f in potential_files\n",
        "        if \"cleaned_\" not in f\n",
        "        and \"_with_scores\" not in f\n",
        "        and \"patient_scores_calculated\" not in f\n",
        "    ]\n",
        "\n",
        "    if not input_files:\n",
        "        print(\"‚ùå No valid input file found! Please upload a .csv or .xlsx file.\")\n",
        "        return\n",
        "\n",
        "    # Select the most recently uploaded file\n",
        "    raw_input_file = max(input_files, key=os.path.getmtime)\n",
        "    print(f\"üìÇ Detected Input File: {raw_input_file}\")\n",
        "\n",
        "    # 2. Run Cleaning\n",
        "    print(f\"--- 1. Cleaning {raw_input_file} ---\")\n",
        "    if subprocess.call(f\"python clean_data.py \\\"{raw_input_file}\\\"\", shell=True) != 0: return\n",
        "\n",
        "    # Determine cleaned filename (matches logic in clean_data.py)\n",
        "    file_root, _ = os.path.splitext(raw_input_file)\n",
        "    cleaned_file = f\"cleaned_{file_root}.csv\"\n",
        "\n",
        "    # 3. Run Scoring\n",
        "    print(f\"--- 2. Scoring {cleaned_file} ---\")\n",
        "    if subprocess.call(f\"python calculate_scores.py \\\"{cleaned_file}\\\"\", shell=True) != 0: return\n",
        "\n",
        "    # Determine scored filename (matches logic in calculate_scores.py)\n",
        "    base_cleaned, ext = os.path.splitext(cleaned_file)\n",
        "    scored_file = f\"{base_cleaned}_with_scores{ext}\"\n",
        "\n",
        "    # 4. Run Preprocessing\n",
        "    print(f\"--- 3. Preprocessing {scored_file} ---\")\n",
        "    if subprocess.call(f\"python preprocess_data.py \\\"{scored_file}\\\"\", shell=True) != 0: return\n",
        "\n",
        "    print(\"\\n‚úÖ Pipeline Finished! 'train_data.jsonl' is ready for fine-tuning.\")\n",
        "\n",
        "run_auto_pipeline()"
      ],
      "metadata": {
        "id": "VJ-ftIuu22j1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30a937a5-ba7f-462c-a441-e3d16272e129"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting Automated Clinical Pipeline...\n",
            "üìÇ Detected Input File: simulated_primed_data_1k.csv\n",
            "--- 1. Cleaning simulated_primed_data_1k.csv ---\n",
            "--- 2. Scoring cleaned_simulated_primed_data_1k.csv ---\n",
            "--- 3. Preprocessing cleaned_simulated_primed_data_1k_with_scores.csv ---\n",
            "\n",
            "‚úÖ Pipeline Finished! 'train_data.jsonl' is ready for fine-tuning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile finetune_clinical.py\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments\n",
        ")\n",
        "from trl import SFTTrainer\n",
        "\n",
        "MODEL_NAME = \"BioMistral/BioMistral-7B\"\n",
        "NEW_MODEL_NAME = \"BioMistral-Clinical-Finetuned\"\n",
        "\n",
        "# Load Data\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": \"train_data.jsonl\", \"validation\": \"val_data.jsonl\"})\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Load Model\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Training Args\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=25,\n",
        "    logging_steps=25,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\"\n",
        ")\n",
        "\n",
        "# SFT Trainer (Simplified)\n",
        "# We removed formatting_func because 'train_data.jsonl' now has a 'text' column\n",
        "# that contains the fully formatted prompt.\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    peft_config=peft_config,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    max_seq_length=1024,\n",
        "    dataset_text_field=\"text\",  # <--- Points to the pre-formatted column\n",
        "    packing=False\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"Saving model...\")\n",
        "trainer.model.save_pretrained(NEW_MODEL_NAME)\n",
        "tokenizer.save_pretrained(NEW_MODEL_NAME)\n",
        "print(f\"Model saved to {NEW_MODEL_NAME}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeXewpvaex0o",
        "outputId": "50d365be-28ae-476b-a27e-57dfad68953c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing finetune_clinical.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python finetune_clinical.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJuhVDyrKV5y",
        "outputId": "9be70aa6-d441-4502-a08e-9c1c18a8d3e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-09 08:13:03.281773: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765267983.315331   14333 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765267983.325653   14333 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765267983.351024   14333 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765267983.351061   14333 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765267983.351067   14333 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765267983.351074   14333 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Generating train split: 900 examples [00:00, 5516.24 examples/s]\n",
            "Generating validation split: 100 examples [00:00, 7077.32 examples/s]\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "tokenizer_config.json: 1.42kB [00:00, 957kB/s]\n",
            "tokenizer.model: 100% 493k/493k [00:00<00:00, 943kB/s]\n",
            "tokenizer.json: 1.80MB [00:00, 72.4MB/s]\n",
            "special_tokens_map.json: 100% 72.0/72.0 [00:00<00:00, 430kB/s]\n",
            "config.json: 100% 567/567 [00:00<00:00, 4.45MB/s]\n",
            "pytorch_model.bin: 100% 14.5G/14.5G [02:26<00:00, 98.5MB/s]\n",
            "generation_config.json: 100% 111/111 [00:00<00:00, 642kB/s]\n",
            "Map: 100% 900/900 [00:08<00:00, 109.19 examples/s]\n",
            "Map: 100% 100/100 [00:00<00:00, 140.60 examples/s]\n",
            "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:469: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "Starting training...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile merge_model.py\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "BASE_MODEL = \"BioMistral/BioMistral-7B\"\n",
        "ADAPTER_DIR = \"BioMistral-Clinical-Finetuned\"\n",
        "OUTPUT_DIR = \"merged_clinical_model\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL, low_cpu_mem_usage=True, return_dict=True, torch_dtype=torch.float16, device_map=\"auto\"\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"Merged model saved to {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "hDicVx0v3C1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python merge_model.py"
      ],
      "metadata": {
        "id": "kpnFcRyE3FXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile agent_interaction.py\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_path = \"merged_clinical_model\"\n",
        "print(f\"Loading model from {model_path}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "\n",
        "def query_clinical_model(input_text):\n",
        "    prompt = (\n",
        "        f\"### Instruction: Analyze the patient clinical data and predict the CTPA result.\\n\"\n",
        "        f\"### Input: {input_text}\\n\"\n",
        "        f\"### Output:\"\n",
        "    )\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=50, temperature=0.1)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"### Output:\")[-1].strip()\n",
        "\n",
        "print(\"\\n--- Clinical Agent Ready ---\")\n",
        "print(\"Enter patient data (e.g., 'Age: 65, HR: 110, Chest Pain: Yes'). Type 'exit' to quit.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Patient Data: \")\n",
        "    if user_input.lower() in ['exit', 'quit']: break\n",
        "\n",
        "    print(\"Analysis: \", query_clinical_model(user_input))\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "kfBxAY_C3HGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python agent_interaction.py"
      ],
      "metadata": {
        "id": "rbfoSMvG3K_2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1f377d89913a44aba7702dfff231a93c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_946299f94a56493c8caa84f68f983e66"
          }
        },
        "9cb99347968f4166ad1903f427fa86cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17b217da67784949b5078635b183be32",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_639c93ebd2214882be8121e5669accd2",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "beafaa40f4bc4804b34fdd04efb3b989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_8d9f633bd46a42a89f0a2bf722e637bb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e2cca4dd92f041faa8f4d50cfadb82cb",
            "value": ""
          }
        },
        "b08ed52ca9414158abec682dc8e42ac7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_c27673b3ffae47a3966c3029c9dce8c6",
            "style": "IPY_MODEL_8b9c25c9c7d648de8b946a5e652c1b3a",
            "value": false
          }
        },
        "f02d187405e4421eaa88f2994743ea7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_382998b08f85435492856ffbf3c3beec",
            "style": "IPY_MODEL_ced9fbb78f98476490b92fbc32e495cf",
            "tooltip": ""
          }
        },
        "ae6d6cb88a6549b088b7c06cded73ae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c86bc5b814684977b42964836656426a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7a195e330a74422c9beea002b4b07454",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "946299f94a56493c8caa84f68f983e66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "17b217da67784949b5078635b183be32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "639c93ebd2214882be8121e5669accd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d9f633bd46a42a89f0a2bf722e637bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2cca4dd92f041faa8f4d50cfadb82cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c27673b3ffae47a3966c3029c9dce8c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b9c25c9c7d648de8b946a5e652c1b3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "382998b08f85435492856ffbf3c3beec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ced9fbb78f98476490b92fbc32e495cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "c86bc5b814684977b42964836656426a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a195e330a74422c9beea002b4b07454": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "394239a6546b48ebb048def1632e9af1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_649bdd0cd3a6421eaab3ef2542aa9289",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f52316e660eb44e18b7ef8fac139bac3",
            "value": "Connecting..."
          }
        },
        "649bdd0cd3a6421eaab3ef2542aa9289": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f52316e660eb44e18b7ef8fac139bac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}